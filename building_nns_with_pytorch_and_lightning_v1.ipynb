{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-trinidad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Mapping' from 'collections' (/Users/pouya/opt/anaconda3/envs/pytorch/lib/python3.10/collections/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m \u001b[39m## matplotlib allows us to draw graphs.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m \u001b[39m## seaborn makes it easier to draw nice-looking graphs.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseed\u001b[39;00m \u001b[39mimport\u001b[39;00m seed_everything \u001b[39m# this is added because people on different computers were\u001b[39;00m\n\u001b[1;32m     13\u001b[0m seed_everything(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/__init__.py:53\u001b[0m\n\u001b[1;32m     50\u001b[0m     sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPartial import of `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` during the build process.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)  \u001b[39m# pragma: no-cover\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[39m# We are not importing the rest of the lightning during the build process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningModule, data_loader\n\u001b[1;32m     54\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback\n\u001b[1;32m     55\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/core/__init__.py:339\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mA :class:`~LightningModule` organizes your PyTorch code into the following sections:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m \n\u001b[1;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m data_loader\n\u001b[1;32m    340\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlightning\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningModule\n\u001b[1;32m    342\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mLightningModule\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata_loader\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/core/decorators.py:6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlightning\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningModule\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m rank_zero_warn\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_loader\u001b[39m(fn):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mimport\u001b[39;00m _logger \u001b[39mas\u001b[39;00m log\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrads\u001b[39;00m \u001b[39mimport\u001b[39;00m GradInformation\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhooks\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelHooks\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelSummary\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelIO, PRIMITIVE_TYPES, ALLOWED_CONFIG_TYPES\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/core/hooks.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Optimizer\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m move_data_to_device, NATIVE_AMP_AVALAIBLE\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mapex\u001b[39;00m \u001b[39mimport\u001b[39;00m amp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/utilities/__init__.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m rank_zero_only, rank_zero_warn, rank_zero_info\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply_func\u001b[39;00m \u001b[39mimport\u001b[39;00m move_data_to_device\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparsing\u001b[39;00m \u001b[39mimport\u001b[39;00m AttributeDict\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/utilities/apply_func.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m ABC\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Mapping, Sequence\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcopy\u001b[39;00m \u001b[39mimport\u001b[39;00m copy\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Union\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Mapping' from 'collections' (/Users/pouya/opt/anaconda3/envs/pytorch/lib/python3.10/collections/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import SGD # optim contains many optimizers. Here, we're using SGD, stochastic gradient descent.\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "import matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\n",
    "import seaborn as sns ## seaborn makes it easier to draw nice-looking graphs.\n",
    "\n",
    "from pytorch_lightning.utilities.seed import seed_everything # this is added because people on different computers were\n",
    "seed_everything(seed=42)                                     # getting different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-guidance",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-confusion",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "# Build a Simple Neural Network in PyTorch\n",
    "\n",
    "Just like building a ***pre-trained*** neural network in **PyTorch**, building a ***pre-trained*** neural network with **PyTorch + Lightning** means creating a new class with two methods: `__init__()` and `forward()`. The `__init__()` method defines and initializes all of the parameters that we want to use, and the `forward()` method tells **PyTorch + Lightning** what should happen during a forward pass through the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a neural network class by creating a class that inherits from LightningModule\n",
    "class BasicLightning(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the weights and biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, L.LightningModule.\n",
    "        \n",
    "        ## Now create the weights and biases that we need for our neural network.\n",
    "        ## Each weight or bias is an nn.Parameter, which gives us the option to optimize the parameter by setting\n",
    "        ## requires_grad, which is short for \"requires gradient\", to True. Since we don't need to optimize any of these\n",
    "        ## parameters now, we set requires_grad=False.\n",
    "        ##\n",
    "        ## NOTE: Because our neural network is already fit to the data, we will input specific values\n",
    "        ## for each weight and bias. In contrast, if we had not already fit the neural network to the data,\n",
    "        ## we might start with a random initalization of the weights and biases.\n",
    "        self.w00 = nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
    "        self.b00 = nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
    "        self.w01 = nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
    "        \n",
    "        self.w10 = nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
    "        self.b10 = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.w11 = nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
    "\n",
    "        self.final_bias = nn.Parameter(torch.tensor(-16.), requires_grad=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, input): ## forward() takes an input value and runs it though the neural network \n",
    "                              ## illustrated at the top of this notebook. \n",
    "        \n",
    "        ## the next three lines implement the top of the neural network (using the top node in the hidden layer).\n",
    "        input_to_top_relu = input * self.w00 + self.b00\n",
    "        top_relu_output = F.relu(input_to_top_relu)\n",
    "        scaled_top_relu_output = top_relu_output * self.w01\n",
    "        \n",
    "        ## the next three lines implement the bottom of the neural network (using the bottom node in the hidden layer).\n",
    "        input_to_bottom_relu = input * self.w10 + self.b10\n",
    "        bottom_relu_output = F.relu(input_to_bottom_relu)\n",
    "        scaled_bottom_relu_output = bottom_relu_output * self.w11\n",
    "        \n",
    "        ## here, we combine both the top and bottom nodes from the hidden layer with the final bias.\n",
    "        input_to_final_relu = (scaled_top_relu_output \n",
    "                               + scaled_bottom_relu_output \n",
    "                               + self.final_bias)\n",
    "        \n",
    "        output = F.relu(input_to_final_relu)\n",
    "    \n",
    "        return output # output is the predicted effectiveness for a drug dose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-retrieval",
   "metadata": {},
   "source": [
    "Once we have created the class that defines the neural network, we can create an actual neural network and print out its parameters, just to make sure things are what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the neural network. \n",
    "model = BasicLightning()\n",
    "\n",
    "## print out the name and value for each parameter\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-carrier",
   "metadata": {},
   "source": [
    "## BAM!!!\n",
    "The values for each weight and bias in `BasicLightning` match the values we see in the optimized neural network (below).\n",
    "<img src=\"./images/simple_relu.001.png\" alt=\"A simple Neural Network\" style=\"width: 810px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-mouth",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-rings",
   "metadata": {},
   "source": [
    "<a id=\"using\"></a>\n",
    "# Use the Neural Network and Graph the Output\n",
    "\n",
    "Now that we have a neural network, we can use it on a variety of doses to determine which will be effective. Then we can make a graph of these data, and this graph should match the green bent shape fit to the training data that's shown at the top of this document. So, let's start by making a sequence of input doses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the different doses we want to run through the neural network.\n",
    "## torch.linspace() creates the sequence of numbers between, and including, 0 and 1.\n",
    "input_doses = torch.linspace(start=0, end=1, steps=11)\n",
    "\n",
    "# now print out the doses to make sure they are what we expect...\n",
    "input_doses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-cylinder",
   "metadata": {},
   "source": [
    "Now that we have `input_doses`, let's run them through the neural network and graph the output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the neural network. \n",
    "model = BasicLightning() \n",
    "\n",
    "## now run the different doses through the neural network.\n",
    "output_values = model(input_doses)\n",
    "\n",
    "## Now draw a graph that shows the effectiveness for each dose.\n",
    "##\n",
    "## First, set the style for seaborn so that the graph looks cool.\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "## create the graph (you might not see it at this point, but you will after we save it as a PDF).\n",
    "sns.lineplot(x=input_doses, \n",
    "             y=output_values, \n",
    "             color='green', \n",
    "             linewidth=2.5)\n",
    "\n",
    "## now label the y- and x-axes.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')\n",
    "\n",
    "## optionally, save the graph as a PDF.\n",
    "# plt.savefig('BasicLightning.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-franchise",
   "metadata": {},
   "source": [
    "The graph shows that the neural network fits the training data. In other words, so far, we don't have any bugs in our code.\n",
    "# Double BAM!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-community",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-member",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# Optimize (Train) a Parameter in the Neural Network and Graph the Output\n",
    "\n",
    "Now that we know how to create and use a simple neural network, and we can graph the output relative to the input, let's see how to train a neural network. The first thing we need to do is tell **Lightning** which parameter (or parameters) we want to train, and we do that by setting `requires_grad=True`. In this example, we'll train `final_bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a neural network class that we can train by creating a class that inherits from LightningModule\n",
    "##\n",
    "## NOTE: This new class, BasicLightningTrain, contains two new methods for training:\n",
    "##\n",
    "## training_step() - This method takes care of 4 things:\n",
    "##      a) calculates the loss for an epoch \n",
    "##      b) resets the gradients \n",
    "##      c) backpropagation \n",
    "##      d) updates the parameters\n",
    "## configure_optimizers() - defines the method we will use to optimize the model\n",
    "class BasicLightningTrain(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the weights and biases.\n",
    "\n",
    "        ## NOTE: The code for __init__ () is the same as before except we now have a learning rate parameter (for \n",
    "        ##       gradient descent) and we modified final_bias in two ways:\n",
    "        ##           1) we set the value of the tensor to 0, and\n",
    "        ##           2) we set \"requires_grad=True\".\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "        \n",
    "        self.w00 = nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
    "        self.b00 = nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
    "        self.w01 = nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
    "        \n",
    "        self.w10 = nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
    "        self.b10 = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.w11 = nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
    "\n",
    "        ## We want to modify final_bias to demonstrate how to optimize it with backpropagation.\n",
    "        ## NOTE: The optimal value for final_bias is -16...\n",
    "#         self.final_bias = nn.Parameter(torch.tensor(-16.), requires_grad=False)\n",
    "        ## ...so we set it to 0 and tell Pytorch that it now needs to calculate the gradient for this parameter.\n",
    "        self.final_bias = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        \n",
    "        self.learning_rate = 0.01 ## this is for gradient descent. NOTE: we will improve this value later, so, technically\n",
    "                                  ## this is just a placeholder until then. In other words, we could put any value here\n",
    "                                  ## because later we will replace it with the improved value.\n",
    "\n",
    "        \n",
    "    def forward(self, input): \n",
    "        \n",
    "        ## forward() is the exact same as before\n",
    "        \n",
    "        input_to_top_relu = input * self.w00 + self.b00\n",
    "        top_relu_output = F.relu(input_to_top_relu)\n",
    "        scaled_top_relu_output = top_relu_output * self.w01\n",
    "        \n",
    "        input_to_bottom_relu = input * self.w10 + self.b10\n",
    "        bottom_relu_output = F.relu(input_to_bottom_relu)\n",
    "        scaled_bottom_relu_output = bottom_relu_output * self.w11\n",
    "        \n",
    "        input_to_final_relu = (scaled_top_relu_output \n",
    "                               + scaled_bottom_relu_output \n",
    "                               + self.final_bias)\n",
    "        \n",
    "        output = F.relu(input_to_final_relu)\n",
    "    \n",
    "        return output # output is the predicted effectiveness for a drug dose.\n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return SGD(self.parameters(), lr=self.learning_rate) # NOTE: We set the learning rate (lr) to our new variable\n",
    "                                                             # self.learning_rate\n",
    "\n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        \n",
    "        ## NOTE: When training_step() is called it calculates the loss with the code below...\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ##...before calling (internally and behind the scenes)...\n",
    "        ## optimizer.zero_grad() # to clear gradients\n",
    "        ## loss.backward() # to do the backpropagation\n",
    "        ## optimizer.step() # to update the parameters\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-designation",
   "metadata": {},
   "source": [
    "Now let's graph the output of `BasicLightningTrain`, which is currently not optimized, and compare it to the graph we drew earlier of the optimized neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the neural network. \n",
    "model = BasicLightningTrain() \n",
    "\n",
    "## now run the different doses through the neural network.\n",
    "output_values = model(input_doses)\n",
    "\n",
    "## Now draw a graph that shows the effectiveness for each dose.\n",
    "##\n",
    "## set the style for seaborn so that the graph looks cool.\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "## create the graph (you might not see it at this point, but you will after we save it as a PDF).\n",
    "sns.lineplot(x=input_doses, \n",
    "             y=output_values.detach(), ## NOTE: because final_bias has a gradident, we call detach() \n",
    "                                       ## to return a new tensor that only has the value and not the gradient.\n",
    "             color='green', \n",
    "             linewidth=2.5)\n",
    "\n",
    "## now label the y- and x-axes.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')\n",
    "\n",
    "## lastly, save the graph as a PDF.\n",
    "#plt.savefig('BasicLightningTrain.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-discharge",
   "metadata": {},
   "source": [
    "The graph shows that when the dose is **0.5**, the output from the unoptimized neural network is **17**, which is way too high, since the output value should be **1**. So, now that we have a parameter we can optimize, let's create some training data that we can use to optimize it. **NOTE:** Instead of just jamming some tensors into our neural network for training, **Lightning** requires us to wrap the training data in a `DataLoader`, which provides a lot of nice features. For example, if we had a large dataset, a `DataLoader` gives us a super easy way to access the data in batches instead of all at once. This is critical when we have more data than RAM to store it in. A `DataLoader` can also shuffle the data for us each epoch and makes it easy to only use a fraction of the data if we want to do a quick and rough training for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "# inputs = torch.tensor([0., 0.5, 1.])\n",
    "# labels = torch.tensor([0., 1., 0.])\n",
    "## NOTE: Because we have so little data, and let's be honest, it's an unrealistically small \n",
    "## amount of data, the learning rate algorithm, lr_find(), that we use in the next section has trouble. \n",
    "## So, the point here is to show how to use lr_find() when you have a reasonable amount of data, \n",
    "## which we fake here by making 100 copies of the inputs and labels.\n",
    "inputs = torch.tensor([0., 0.5, 1.] * 100)\n",
    "labels = torch.tensor([0., 1., 0.] * 100)\n",
    "\n",
    "## If we want to use Lightning for training, then we have to pass the Trainer the data wrapped in \n",
    "## something called a DataLoader. DataLoaders provide a handful of nice features including...\n",
    "##   1) They can access the data in minibatches instead of all at once. In other words,\n",
    "##      The DataLoader doesn't need us to load all of the data into memory first. Instead\n",
    "##      it just loads what it needs in an efficient way. This is crucial for large datasets.\n",
    "##   2) They can reshuffle the data every epoch to reduce model overfitting\n",
    "##   3) We can easily just use a fraction of the data if we want do a quick train\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-control",
   "metadata": {},
   "source": [
    "And now that we have some training data, the first thing we need to do is find the optimal **Learning Rate** for gradient descent. We do this by creating a **Lightning** `Trainer` and using it to call `tuner.lr_find()` to find an improved learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df12150-2da7-495a-b16d-a4f28813ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicLightningTrain() # First, make model from the class\n",
    "\n",
    "## Now create a Trainer - we can use the trainer to...\n",
    "##  1) Find the optimal learning rate\n",
    "##  2) Train (optimize) the weights and biases in the model\n",
    "## By default, the trainer will run on your system's CPU\n",
    "trainer = L.Trainer(max_epochs=20) \n",
    "## However, if we wanted to automatically take advantage of any available GPUs,\n",
    "## we would set accelerator=\"auto\" to automatically use available GPUs\n",
    "## and we would set devices=\"auto\" to automatically select as many GPUs as we have.\n",
    "#\n",
    "# trainer = L.Trainer(max_epochs=34, accelerator=\"auto\", devices=\"auto\")\n",
    "\n",
    "## Now let's find the optimal learning rate\n",
    "lr_find_results = trainer.tuner.lr_find(model,\n",
    "                                        train_dataloaders=dataloader, # the training data\n",
    "                                        min_lr=0.001, # minimum learning rate\n",
    "                                        max_lr=1.0,   # maximum learning rate\n",
    "                                        early_stop_threshold=None) # setting this to \"None\" tests all 100 candidate rates\n",
    "new_lr = lr_find_results.suggestion() ## suggestion() returns the best guess for the optimal learning rate\n",
    "\n",
    "## now print out the learning rate\n",
    "print(f\"lr_find() suggests {new_lr:.5f} for the learning rate.\")\n",
    "\n",
    "# now set the model's learning rate to the new value\n",
    "model.learning_rate = new_lr\n",
    "\n",
    "## NOTE: we can also plot the loss for each learning rate tested.\n",
    "## When you have a lot of data, this graph can be useful\n",
    "## (see https://pytorch-lightning.readthedocs.io/en/1.4.5/advanced/lr_finder.html to learn how to interpret)\n",
    "## but when you only have 3 data points, like our example, this plot is pretty hard to interpret so I did\n",
    "## not cover it in the video.\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f0cd2-e79a-4b76-a852-3629fbdc4ce9",
   "metadata": {},
   "source": [
    "Now that we have an improved training rate, let's train the model to optimize `final_bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b031b-64d6-4b45-8bf4-f3f240a03071",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have an improved learning rate, we can train the model (optimize final_bias)\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(model.final_bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-settlement",
   "metadata": {},
   "source": [
    "So, if everything worked correctly, the optimizer should have converged on `final_bias = 16.0070`. **BAM!**\n",
    "\n",
    "Lastly, let's graph the output from the optimized neural network and see if it's the same as what we started with. If so, then the optimization worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the different doses through the neural network\n",
    "output_values = model(input_doses)\n",
    "\n",
    "## set the style for seaborn so that the graph looks cool.\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "## create the graph (you might not see it at this point, but you will after we save it as a PDF).\n",
    "sns.lineplot(x=input_doses, \n",
    "             y=output_values.detach(), ## NOTE: we call detach() because final_bias has a gradient\n",
    "             color='green', \n",
    "             linewidth=2.5)\n",
    "\n",
    "## now label the y- and x-axes.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')\n",
    "\n",
    "## lastly, save the graph as a PDF.\n",
    "# plt.savefig('BasicLightningTrain_optimized.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-broadcast",
   "metadata": {},
   "source": [
    "And we see that the optimized model results in the same graph that we started with, so the optimization worked as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-narrative",
   "metadata": {},
   "source": [
    "# Triple BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e95d3e192d2ef436c3eb601813d0c157996a00c0a111ae24ae11f05ea9794015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
